use std::collections::{HashMap, HashSet};
use std::fmt::Debug;
use std::sync::Arc;
use std::time::Duration;

use aws_sdk_ec2::error::DescribeInstancesError;
use aws_sdk_ec2::model::Filter;
use aws_sdk_ec2::output::DescribeInstancesOutput;
use aws_sdk_ec2::types::SdkError;
use aws_sdk_ec2::Client as Ec2Client;
use aws_sdk_servicequotas::model::ServiceQuota;
use aws_sdk_servicequotas::Client as ServiceQuotaClient;
use aws_smithy_http::endpoint::Endpoint as AWSEndpoint;
use futures::stream::StreamExt;
use json_patch::{PatchOperation, RemoveOperation, TestOperation};
use k8s_openapi::api::core::v1::{Node, Pod};
use k8s_openapi::apiextensions_apiserver::pkg::apis::apiextensions::v1::CustomResourceDefinition;
use kube::api::{Api, DeleteParams, ListParams, Patch, PatchParams, PostParams};
use kube::core::crd::merge_crds;
use kube::{Client, CustomResourceExt, Resource, ResourceExt};
use kube_runtime::controller::{Action, Controller};
use kube_runtime::finalizer::{finalizer, Event};
use kube_runtime::wait::{await_condition, conditions};
use rand::{thread_rng, Rng};
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use tokio::join;
use tokio::task;
use tracing::{debug, event, info, instrument, Level};

use eip_operator_shared::{run_with_tracing, Error, MANAGE_EIP_LABEL};

use v2::{Eip, EipSelector, EipSpec};

mod eip;

const LEGACY_MANAGE_EIP_LABEL: &str = "eip.aws.materialize.com/manage";
const LEGACY_POD_FINALIZER_NAME: &str = "eip.aws.materialize.com/disassociate";

const FIELD_MANAGER: &str = "eip.materialize.cloud";
const AUTOCREATE_EIP_LABEL: &str = "eip.materialize.cloud/autocreate_eip";
const POD_FINALIZER_NAME: &str = "eip.materialize.cloud/disassociate";
const NODE_FINALIZER_NAME: &str = "eip.materialize.cloud/disassociate_node";
const EIP_API_VERSION: &str = "materialize.cloud/v1";
const EIP_FINALIZER_NAME: &str = "eip.materialize.cloud/destroy";
const EIP_ALLOCATION_ID_ANNOTATION: &str = "eip.materialize.cloud/allocation_id";
const EXTERNAL_DNS_TARGET_ANNOTATION: &str = "external-dns.alpha.kubernetes.io/target";

// See https://us-east-1.console.aws.amazon.com/servicequotas/home/services/ec2/quotas
// and filter in the UI for EC2 quotas like this, or use the CLI:
//   aws --profile=mz-cloud-staging-admin service-quotas list-service-quotas --service-code=ec2
const EIP_QUOTA_CODE: &str = "L-0263D0A3";

// Watch our EIP quota status on a fixed interval
const EIP_QUOTA_INTERVAL: tokio::time::Duration = Duration::from_secs(60);

struct ContextData {
    cluster_name: String,
    namespace: Option<String>,
    default_tags: HashMap<String, String>,
    k8s_client: Client,
    ec2_client: Ec2Client,
}

impl std::fmt::Debug for ContextData {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("ContextData")
            .field("cluster_name", &self.cluster_name)
            .field("namespace", &self.namespace)
            .field("default_tags", &self.default_tags)
            .finish()
    }
}

impl ContextData {
    fn new(
        cluster_name: String,
        namespace: Option<String>,
        default_tags: HashMap<String, String>,
        k8s_client: Client,
        ec2_client: Ec2Client,
    ) -> ContextData {
        ContextData {
            cluster_name,
            namespace,
            default_tags,
            k8s_client,
            ec2_client,
        }
    }
}

mod v1 {
    use kube::CustomResource;
    use schemars::JsonSchema;
    use serde::{Deserialize, Serialize};

    use super::EipStatus;

    /// The spec for the Eip Kubernetes custom resource.
    /// An `Eip` type is generated by deriving `CustomResource`.
    #[derive(CustomResource, Clone, Debug, Deserialize, Serialize, JsonSchema)]
    #[serde(rename_all = "camelCase")]
    #[kube(
        group = "materialize.cloud",
        version = "v1",
        kind = "Eip",
        singular = "eip",
        plural = "eips",
        namespaced,
        status = "EipStatus",
        printcolumn = r#"{"name": "AllocationID", "type": "string", "description": "Allocation ID of the EIP.", "jsonPath": ".status.allocationId"}"#,
        printcolumn = r#"{"name": "PublicIP", "type": "string", "description": "Public IP address of the EIP.", "jsonPath": ".status.publicIpAddress"}"#,
        printcolumn = r#"{"name": "Pod", "type": "string", "description": "Pod name to associate the EIP with.", "jsonPath": ".spec.podName", "priority": 1}"#,
        printcolumn = r#"{"name": "ENI", "type": "string", "description": "ID of the Elastic Network Interface of the pod.", "jsonPath": ".status.eni", "priority": 1}"#,
        printcolumn = r#"{"name": "PrivateIP", "type": "string", "description": "Private IP address of the pod.", "jsonPath": ".status.privateIpAddress", "priority": 1}"#
    )]
    pub(crate) struct EipSpec {
        pub(crate) pod_name: String,
    }
}

mod v2 {
    use kube::CustomResource;
    use schemars::JsonSchema;
    use serde::{Deserialize, Serialize};
    use std::collections::BTreeMap;

    use super::EipStatus;

    #[derive(Clone, Debug, Deserialize, Serialize, JsonSchema)]
    #[serde(rename_all = "camelCase")]
    pub(crate) enum EipSelector {
        #[serde(rename_all = "camelCase")]
        Pod { pod_name: String },
        #[serde(rename_all = "camelCase")]
        Node { selector: BTreeMap<String, String> },
    }

    impl std::fmt::Display for EipSelector {
        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> Result<(), std::fmt::Error> {
            match self {
                Self::Pod { pod_name } => {
                    write!(f, "Pod({})", pod_name)
                }
                Self::Node { selector } => {
                    write!(f, "Node(")?;
                    let mut first = true;
                    for label in selector {
                        if !first {
                            write!(f, ", ")?;
                            first = false;
                        }
                        write!(f, "{}: {}", label.0, label.1)?;
                    }
                    write!(f, ")")?;
                    Ok(())
                }
            }
        }
    }

    /// The spec for the Eip Kubernetes custom resource.
    /// An `Eip` type is generated by deriving `CustomResource`.
    #[derive(CustomResource, Clone, Debug, Deserialize, Serialize, JsonSchema)]
    #[serde(rename_all = "camelCase")]
    #[kube(
        group = "materialize.cloud",
        version = "v2",
        kind = "Eip",
        singular = "eip",
        plural = "eips",
        namespaced,
        status = "EipStatus",
        printcolumn = r#"{"name": "AllocationID", "type": "string", "description": "Allocation ID of the EIP.", "jsonPath": ".status.allocationId"}"#,
        printcolumn = r#"{"name": "PublicIP", "type": "string", "description": "Public IP address of the EIP.", "jsonPath": ".status.publicIpAddress"}"#,
        printcolumn = r#"{"name": "Selector", "type": "string", "description": "Selector for the pod or node to associate the EIP with.", "jsonPath": ".spec.selector", "priority": 1}"#,
        printcolumn = r#"{"name": "ENI", "type": "string", "description": "ID of the Elastic Network Interface of the pod.", "jsonPath": ".status.eni", "priority": 1}"#,
        printcolumn = r#"{"name": "PrivateIP", "type": "string", "description": "Private IP address of the pod.", "jsonPath": ".status.privateIpAddress", "priority": 1}"#
    )]
    pub(crate) struct EipSpec {
        pub(crate) selector: EipSelector,
    }

    impl Eip {
        pub fn pod_name(&self) -> Option<&str> {
            match &self.spec.selector {
                EipSelector::Pod { pod_name } => Some(pod_name.as_ref()),
                _ => None,
            }
        }
    }
}

/// The status fields for the Eip Kubernetes custom resource.
#[derive(Clone, Debug, Deserialize, Serialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct EipStatus {
    allocation_id: Option<String>,
    public_ip_address: Option<String>,
    eni: Option<String>,
    private_ip_address: Option<String>,
}

/// Registers the Eip custom resource with Kubernetes,
/// the specification of which is automatically derived from the structs.
#[instrument(skip(k8s_client), err, fields(crd_data))]
async fn register_eip_custom_resource(
    k8s_client: Client,
    namespace: Option<&str>,
) -> Result<(), Error> {
    // https://github.com/kube-rs/kube-rs/blob/master/examples/crd_derive_schema.rs#L224
    let crd_api = Api::<CustomResourceDefinition>::all(k8s_client.clone());
    let data = merge_crds(vec![v1::Eip::crd(), v2::Eip::crd()], "v2").unwrap();
    let crd_json = serde_json::to_string(&data)?;
    event!(Level::INFO, crd_json = %crd_json);
    let crd_patch = Patch::Apply(data);
    crd_api
        .patch(
            "eips.materialize.cloud",
            &PatchParams::apply(FIELD_MANAGER),
            &crd_patch,
        )
        .await?;
    let establish = await_condition(
        crd_api.clone(),
        "eips.materialize.cloud",
        conditions::is_crd_established(),
    );
    tokio::time::timeout(std::time::Duration::from_secs(10), establish).await??;

    let ar = kube::api::ApiResource::erase::<v1::Eip>(&());
    #[derive(Clone, Serialize, Deserialize, Debug)]
    #[serde(rename_all = "camelCase")]
    struct MaybeEipSpec {
        pod_name: Option<String>,
    }
    type MaybeEip = kube::api::Object<MaybeEipSpec, kube::api::NotUsed>;

    let eip_v1_api = match namespace {
        Some(namespace) => Api::<MaybeEip>::namespaced_with(k8s_client.clone(), namespace, &ar),
        None => Api::<MaybeEip>::all_with(k8s_client.clone(), &ar),
    };
    for eip_v1 in eip_v1_api.list(&ListParams::default()).await? {
        if let Some(pod_name) = &eip_v1.spec.pod_name {
            event!(
                Level::INFO,
                eip_v1 = serde_json::to_string(&eip_v1)?,
                "updating existing eip to latest version"
            );
            let name = eip_v1.metadata.name.as_ref().ok_or(Error::MissingEipName)?;
            let eip_api = Api::<Eip>::namespaced(
                k8s_client.clone(),
                eip_v1.metadata.namespace.as_deref().unwrap_or("default"),
            );
            let mut eip = Eip::new(
                name,
                EipSpec {
                    selector: EipSelector::Pod {
                        pod_name: pod_name.to_string(),
                    },
                },
            );
            eip.meta_mut().resource_version = eip_v1.metadata.resource_version;
            eip_api.replace(name, &PostParams::default(), &eip).await?;
        }
    }
    Ok(())
}

/// Applies annotation to pod specifying the target IP for external-dns.
#[instrument(skip(pod_api), err)]
async fn add_dns_target_annotation(
    pod_api: &Api<Pod>,
    pod_name: String,
    eip_address: String,
    allocation_id: String,
) -> Result<Pod, kube::Error> {
    let patch = serde_json::json!({
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {
            "annotations": {
                EIP_ALLOCATION_ID_ANNOTATION: allocation_id,
                EXTERNAL_DNS_TARGET_ANNOTATION: eip_address
            }
        }
    });
    let patch = Patch::Apply(&patch);
    let params = PatchParams::apply(FIELD_MANAGER);
    pod_api.patch(&pod_name, &params, &patch).await
}

/// Sets the allocationId and publicIpAddress fields in the Eip status.
#[instrument(skip(eip_api), err)]
async fn set_eip_status_created(
    eip_api: &Api<Eip>,
    eip_name: &str,
    allocation_id: String,
    public_ip_address: String,
) -> Result<Eip, kube::Error> {
    event!(Level::INFO, "Updating status for created EIP.");
    let patch = serde_json::json!({
        "apiVersion": EIP_API_VERSION,
        "kind": "Eip",
        "status": {
            "allocationId": allocation_id,
            "publicIpAddress": public_ip_address,
        }
    });
    let patch = Patch::Merge(&patch);
    let params = PatchParams::default();
    let result = eip_api.patch_status(eip_name, &params, &patch).await;
    if result.is_ok() {
        event!(Level::INFO, "Done updating status for created EIP.");
    }
    result
}

/// Sets the eni and privateIpAddress fields in the Eip status.
#[instrument(skip(eip_api), err)]
async fn set_eip_status_attached(
    eip_api: &Api<Eip>,
    eip_name: &str,
    eni: String,
    private_ip_address: String,
) -> Result<Eip, kube::Error> {
    event!(Level::INFO, "Updating status for attached EIP.");
    let patch = serde_json::json!({
        "apiVersion": EIP_API_VERSION,
        "kind": "Eip",
        "status": {
            "eni": eni,
            "privateIpAddress": private_ip_address,
        }
    });
    let patch = Patch::Merge(&patch);
    let params = PatchParams::default();
    let result = eip_api.patch_status(eip_name, &params, &patch).await;
    if result.is_ok() {
        event!(Level::INFO, "Done updating status for attached EIP.");
    }
    result
}

/// Unsets the eni and privateIpAddress fields in the Eip status.
#[instrument(skip(eip_api), err)]
async fn set_eip_status_detached(eip_api: &Api<Eip>, eip_name: &str) -> Result<Eip, kube::Error> {
    event!(Level::INFO, "Updating status for detached EIP.");
    let patch = serde_json::json!({
        "apiVersion": EIP_API_VERSION,
        "kind": "Eip",
        "status": {
            "eni": None::<String>,
            "privateIpAddress": None::<String>,
        }
    });
    let patch = Patch::Merge(&patch);
    let params = PatchParams::default();
    let result = eip_api.patch_status(eip_name, &params, &patch).await;
    if result.is_ok() {
        event!(Level::INFO, "Done updating status for detached EIP.");
    }
    result
}

/// Describes an AWS EC2 instance with the supplied instance_id.
#[instrument(skip(ec2_client), err)]
async fn describe_instance(
    ec2_client: &Ec2Client,
    instance_id: String,
) -> Result<DescribeInstancesOutput, SdkError<DescribeInstancesError>> {
    ec2_client
        .describe_instances()
        .instance_ids(instance_id)
        .send()
        .await
}

/// An annotation attached to a pod by EKS describing the branch network interfaces when using per-pod security groups.
/// example: [{"eniId":"eni-0e42914a33ee3c5ce","ifAddress":"0e:cb:3c:0d:97:3b","privateIp":"10.1.191.190","vlanId":1,"subnetCidr":"10.1.160.0/19"}]
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
struct EniDescription {
    eni_id: String,
}

/// Parse the vpc.amazonaws.com/pod-eni annotation if it exists, and return the ENI ID.
#[instrument(skip(pod))]
fn get_eni_id_from_annotation(pod: &Pod) -> Option<String> {
    event!(Level::INFO, "Getting ENI ID from annotation.");
    let annotation = pod
        .metadata
        .annotations
        .as_ref()?
        .get("vpc.amazonaws.com/pod-eni")?;
    event!(Level::INFO, annotation = %annotation);
    let eni_descriptions: Vec<EniDescription> = serde_json::from_str(annotation).ok()?;
    Some(eni_descriptions.first()?.eni_id.to_owned())
}

/// Checks if the autocreate label is set to true on a pod.
fn should_autocreate_eip(pod: &Pod) -> bool {
    pod.metadata
        .labels
        .as_ref()
        .and_then(|label| label.get(AUTOCREATE_EIP_LABEL).map(|s| (*s).as_ref()))
        .unwrap_or("false")
        .to_lowercase()
        == "true"
}

/// Creates a K8S Eip resource.
#[instrument(skip(eip_api), err)]
async fn create_k8s_pod_eip(eip_api: &Api<Eip>, pod_name: &str) -> Result<Eip, kube::Error> {
    //info!("Applying K8S Eip: {}", pod_name);
    let patch = Eip::new(
        pod_name,
        EipSpec {
            selector: EipSelector::Pod {
                pod_name: pod_name.to_owned(),
            },
        },
    );
    let patch = Patch::Apply(&patch);
    let params = PatchParams::apply(FIELD_MANAGER);
    eip_api.patch(pod_name, &params, &patch).await
}

/// Deletes a K8S Eip resource, if it exists.
#[instrument(skip(eip_api), err)]
async fn delete_k8s_eip(eip_api: &Api<Eip>, name: &str) -> Result<(), kube::Error> {
    //info!("Deleting K8S Eip: {}", name);
    match eip_api.delete(name, &DeleteParams::default()).await {
        Ok(_) => Ok(()),
        Err(kube::Error::Api(e)) if e.code == 404 => {
            info!("Eip already deleted: {}", name);
            Ok(())
        }
        Err(e) => Err(e),
    }
}

/// Creates or updates EIP associations when creating or updating a pod.
#[instrument(skip(ec2_client, node_api, eip_api, pod_api, pod), err)]
async fn apply_pod(
    ec2_client: &Ec2Client,
    node_api: &Api<Node>,
    eip_api: &Api<Eip>,
    pod_api: &Api<Pod>,
    pod: Arc<Pod>,
) -> Result<Action, Error> {
    let pod_name = pod.metadata.name.as_ref().ok_or(Error::MissingPodName)?;
    event!(Level::INFO, pod_name = %pod_name, "Applying pod.");
    if should_autocreate_eip(&pod) {
        event!(Level::INFO, should_autocreate_eip = true);
        create_k8s_pod_eip(eip_api, pod_name).await?;
    }
    let pod_ip = pod
        .status
        .as_ref()
        .ok_or(Error::MissingPodIp)?
        .pod_ip
        .as_ref()
        .ok_or(Error::MissingPodIp)?;

    let node_name = pod
        .spec
        .as_ref()
        .ok_or(Error::MissingNodeName)?
        .node_name
        .as_ref()
        .ok_or(Error::MissingNodeName)?;

    let node = node_api.get(node_name).await?;

    let provider_id = node
        .spec
        .as_ref()
        .ok_or(Error::MissingProviderId)?
        .provider_id
        .as_ref()
        .ok_or(Error::MissingProviderId)?;

    let instance_id = provider_id
        .rsplit_once('/')
        .ok_or(Error::MalformedProviderId)?
        .1;

    let eni_id = match get_eni_id_from_annotation(&pod) {
        Some(eni_id) => eni_id,
        None => {
            let instance_description =
                describe_instance(ec2_client, instance_id.to_owned()).await?;

            instance_description
                .reservations
                .as_ref()
                .ok_or(Error::MissingReservations)?[0]
                .instances
                .as_ref()
                .ok_or(Error::MissingInstances)?[0]
                .network_interfaces
                .as_ref()
                .ok_or(Error::MissingNetworkInterfaces)?
                .iter()
                .find_map(|nic| {
                    nic.private_ip_addresses.as_ref()?.iter().find_map(|ip| {
                        match ip.private_ip_address.as_ref()? {
                            x if x == pod_ip => {
                                debug!(
                                    "Found matching NIC: {} {} {}",
                                    nic.network_interface_id.as_ref()?,
                                    pod_ip,
                                    ip.private_ip_address.as_ref()?,
                                );
                                Some(nic.network_interface_id.as_ref()?.to_owned())
                            }
                            _ => None,
                        }
                    })
                })
                .ok_or(Error::NoInterfaceWithThatIp)?
        }
    };

    let all_eips = eip_api.list(&ListParams::default()).await?.items;
    let eip = all_eips
        .into_iter()
        .find(|eip| eip.pod_name() == Some(pod_name))
        .ok_or_else(|| Error::NoEipResourceWithThatPodName(pod_name.to_owned()))?;
    let eip_name = eip.metadata.name.as_ref().ok_or(Error::MissingEipName)?;
    let allocation_id = eip
        .status
        .as_ref()
        .ok_or(Error::MissingEipStatus)?
        .allocation_id
        .as_ref()
        .ok_or(Error::MissingAllocationId)?;
    let eip_description = eip::describe_address(ec2_client, allocation_id.to_owned())
        .await?
        .addresses
        .ok_or(Error::MissingAddresses)?
        .swap_remove(0);
    let public_ip = eip_description.public_ip.ok_or(Error::MissingPublicIp)?;
    if eip_description.network_interface_id != Some(eni_id.to_owned())
        || eip_description.private_ip_address != Some(pod_ip.to_owned())
    {
        eip::associate_eip(
            ec2_client,
            allocation_id.to_owned(),
            eni_id.to_owned(),
            pod_ip.to_owned(),
        )
        .await?;
    }
    set_eip_status_attached(eip_api, eip_name, eni_id, pod_ip.to_owned()).await?;
    add_dns_target_annotation(
        pod_api,
        pod_name.to_owned(),
        public_ip,
        allocation_id.to_owned(),
    )
    .await?;
    Ok(Action::requeue(Duration::from_secs(
        thread_rng().gen_range(240..360),
    )))
}

#[instrument(skip(ec2_client, eip_api, node), err)]
async fn apply_node(
    ec2_client: &Ec2Client,
    eip_api: &Api<Eip>,
    node: Arc<Node>,
) -> Result<Action, Error> {
    let node_name = node.metadata.name.as_ref().ok_or(Error::MissingNodeName)?;
    event!(Level::INFO, node_name = %node_name, "Applying node.");
    let node_ip = &node
        .status
        .as_ref()
        .ok_or(Error::MissingNodeIp)?
        .addresses
        .as_ref()
        .ok_or(Error::MissingNodeIp)?
        .iter()
        .find(|addr| addr.type_ == "InternalIP")
        .ok_or(Error::MissingNodeIp)?
        .address;
    let node_labels = node
        .metadata
        .labels
        .as_ref()
        .ok_or(Error::MissingNodeLabels)?;
    let provider_id = node
        .spec
        .as_ref()
        .ok_or(Error::MissingProviderId)?
        .provider_id
        .as_ref()
        .ok_or(Error::MissingProviderId)?;
    let instance_id = provider_id
        .rsplit_once('/')
        .ok_or(Error::MalformedProviderId)?
        .1;
    let all_eips = eip_api.list(&ListParams::default()).await?.items;
    let eip = all_eips
        .into_iter()
        .find(|eip| match eip.spec.selector {
            EipSelector::Pod { .. } => false,
            EipSelector::Node { ref selector } => {
                for (key, value) in selector {
                    match node_labels.get(key) {
                        Some(node_value) => {
                            if value != node_value {
                                return false;
                            }
                        }
                        None => return false,
                    }
                }
                true
            }
        })
        .ok_or(Error::NoEipResourceWithThatNodeSelector)?;
    let eip_name = eip.metadata.name.as_ref().ok_or(Error::MissingEipName)?;
    let allocation_id = eip
        .status
        .as_ref()
        .ok_or(Error::MissingEipStatus)?
        .allocation_id
        .as_ref()
        .ok_or(Error::MissingAllocationId)?;
    let eip_description = eip::describe_address(ec2_client, allocation_id.to_owned())
        .await?
        .addresses
        .ok_or(Error::MissingAddresses)?
        .swap_remove(0);
    let instance_description = describe_instance(ec2_client, instance_id.to_owned()).await?;

    let eni_id = instance_description
        .reservations
        .as_ref()
        .ok_or(Error::MissingReservations)?[0]
        .instances
        .as_ref()
        .ok_or(Error::MissingInstances)?[0]
        .network_interfaces
        .as_ref()
        .ok_or(Error::MissingNetworkInterfaces)?
        .iter()
        .find_map(|nic| {
            nic.private_ip_addresses.as_ref()?.iter().find_map(|ip| {
                match ip.private_ip_address.as_ref()? {
                    x if x == node_ip => {
                        debug!(
                            "Found matching NIC: {} {} {}",
                            nic.network_interface_id.as_ref()?,
                            node_ip,
                            ip.private_ip_address.as_ref()?,
                        );
                        Some(nic.network_interface_id.as_ref()?.to_owned())
                    }
                    _ => None,
                }
            })
        })
        .ok_or(Error::NoInterfaceWithThatIp)?;
    if eip_description.network_interface_id != Some(eni_id.to_owned())
        || eip_description.private_ip_address != Some(node_ip.to_owned())
    {
        eip::associate_eip(
            ec2_client,
            allocation_id.to_owned(),
            eni_id.to_owned(),
            node_ip.to_owned(),
        )
        .await?;
    }
    set_eip_status_attached(eip_api, eip_name, eni_id, node_ip.to_owned()).await?;

    Ok(Action::requeue(Duration::from_secs(
        thread_rng().gen_range(240..360),
    )))
}

#[instrument(skip(ec2_client, eip_api, eip), err)]
async fn apply_eip(
    ec2_client: &Ec2Client,
    eip_api: &Api<Eip>,
    eip: Arc<Eip>,
    cluster_name: &str,
    namespace: &str,
    default_tags: &HashMap<String, String>,
) -> Result<Action, Error> {
    let eip_uid = eip.metadata.uid.as_ref().ok_or(Error::MissingEipUid)?;
    let eip_name = eip.metadata.name.as_ref().ok_or(Error::MissingEipName)?;
    let eip_selector = &eip.spec.selector;
    event!(Level::INFO, %eip_uid, %eip_name, %eip_selector, "Applying EIP.");
    let addresses =
        eip::describe_addresses_with_tag_value(ec2_client, eip::EIP_UID_TAG, eip_uid.to_owned())
            .await?
            .addresses
            .ok_or(Error::MissingAddresses)?;
    let (allocation_id, public_ip) = match addresses.len() {
        0 => {
            let response = eip::allocate_address(
                ec2_client,
                eip_uid,
                eip_name,
                eip_selector,
                cluster_name,
                namespace,
                default_tags,
            )
            .await?;
            let allocation_id = response.allocation_id.ok_or(Error::MissingAllocationId)?;
            let public_ip = response.public_ip.ok_or(Error::MissingPublicIp)?;
            (allocation_id, public_ip)
        }
        1 => {
            let allocation_id = addresses[0]
                .allocation_id
                .as_ref()
                .ok_or(Error::MissingAllocationId)?;
            let public_ip = addresses[0]
                .public_ip
                .as_ref()
                .ok_or(Error::MissingPublicIp)?;
            (allocation_id.to_owned(), public_ip.to_owned())
        }
        _ => {
            return Err(Error::MultipleEipsTaggedForPod);
        }
    };
    set_eip_status_created(eip_api, eip_name, allocation_id, public_ip).await?;
    Ok(Action::requeue(Duration::from_secs(
        thread_rng().gen_range(240..360),
    )))
}

/// Deletes AWS Elastic IP associated with a pod being destroyed.
#[instrument(skip(ec2_client, eip_api, pod), err)]
async fn cleanup_pod(
    ec2_client: &Ec2Client,
    eip_api: &Api<Eip>,
    pod: Arc<Pod>,
) -> Result<Action, Error> {
    let pod_name = pod.metadata.name.as_ref().ok_or(Error::MissingPodUid)?;
    event!(Level::INFO, pod_name = %pod_name, "Cleaning up pod.");
    let all_eips = eip_api.list(&ListParams::default()).await?.items;
    let eip = all_eips
        .into_iter()
        .find(|eip| eip.pod_name() == Some(pod_name));
    if let Some(eip) = eip {
        let allocation_id = eip
            .status
            .as_ref()
            .ok_or(Error::MissingEipStatus)?
            .allocation_id
            .as_ref()
            .ok_or(Error::MissingAllocationId)?;
        let addresses = eip::describe_address(ec2_client, allocation_id.to_owned())
            .await?
            .addresses
            .ok_or(Error::MissingAddresses)?;
        for address in addresses {
            if let Some(association_id) = address.association_id {
                eip::disassociate_eip(ec2_client, association_id).await?;
            }
        }
        set_eip_status_detached(
            eip_api,
            eip.metadata.name.as_ref().ok_or(Error::MissingEipName)?,
        )
        .await?;
    };
    if should_autocreate_eip(&pod) {
        event!(Level::INFO, should_autocreate_eip = true);
        delete_k8s_eip(eip_api, pod_name).await?;
    }
    Ok(Action::await_change())
}

#[instrument(skip(ec2_client, eip_api, node), err)]
async fn cleanup_node(
    ec2_client: &Ec2Client,
    eip_api: &Api<Eip>,
    node: Arc<Node>,
) -> Result<Action, Error> {
    let node_name = node.metadata.name.as_ref().ok_or(Error::MissingNodeName)?;
    event!(Level::INFO, node_name = %node_name, "Cleaning up node.");
    let node_labels = node
        .metadata
        .labels
        .as_ref()
        .ok_or(Error::MissingNodeLabels)?;
    let all_eips = eip_api.list(&ListParams::default()).await?.items;
    let eip = all_eips
        .into_iter()
        .filter(|eip| {
            eip.status
                .as_ref()
                .map_or(false, |status| status.private_ip_address.is_some())
        })
        .find(|eip| match eip.spec.selector {
            EipSelector::Pod { .. } => false,
            EipSelector::Node { ref selector } => {
                for (key, value) in selector {
                    match node_labels.get(key) {
                        Some(node_value) => {
                            if value != node_value {
                                return false;
                            }
                        }
                        None => return false,
                    }
                }
                true
            }
        });
    if let Some(eip) = eip {
        let allocation_id = eip
            .status
            .as_ref()
            .ok_or(Error::MissingEipStatus)?
            .allocation_id
            .as_ref()
            .ok_or(Error::MissingAllocationId)?;
        let addresses = eip::describe_address(ec2_client, allocation_id.to_owned())
            .await?
            .addresses
            .ok_or(Error::MissingAddresses)?;
        for address in addresses {
            if let Some(association_id) = address.association_id {
                eip::disassociate_eip(ec2_client, association_id).await?;
            }
        }
        set_eip_status_detached(
            eip_api,
            eip.metadata.name.as_ref().ok_or(Error::MissingEipName)?,
        )
        .await?;
    }
    Ok(Action::await_change())
}

#[instrument(skip(ec2_client, eip), err)]
async fn cleanup_eip(ec2_client: &Ec2Client, eip: Arc<Eip>) -> Result<Action, Error> {
    let eip_name = eip.metadata.name.as_ref().ok_or(Error::MissingEipName)?;
    let eip_uid = eip.metadata.uid.as_ref().ok_or(Error::MissingEipUid)?;
    event!(Level::INFO, eip_name = %eip_name, eip_uid = %eip_uid, "Cleaning up eip.");
    let addresses =
        eip::describe_addresses_with_tag_value(ec2_client, eip::EIP_UID_TAG, eip_uid.to_owned())
            .await?
            .addresses;
    if let Some(addresses) = addresses {
        for address in addresses {
            eip::disassociate_and_release_address(ec2_client, &address).await?;
        }
    }
    Ok(Action::await_change())
}

/// Finds all EIPs tagged for this cluster, then compares them to the pod UIDs. If the EIP is not
/// tagged with a pod UID, or the UID does not exist in this cluster, it deletes the EIP.
#[instrument(skip(ec2_client, eip_api, pod_api), err)]
async fn cleanup_orphan_eips(
    ec2_client: &Ec2Client,
    eip_api: &Api<Eip>,
    pod_api: &Api<Pod>,
    cluster_name: &str,
    namespace: Option<&str>,
) -> Result<(), Error> {
    let mut describe_addresses = ec2_client.describe_addresses().filters(
        Filter::builder()
            .name(format!("tag:{}", eip::CLUSTER_NAME_TAG))
            .values(cluster_name.to_owned())
            .build(),
    );
    if let Some(namespace) = namespace {
        describe_addresses = describe_addresses.filters(
            Filter::builder()
                .name(format!("tag:{}", eip::NAMESPACE_TAG))
                .values(namespace.to_owned())
                .build(),
        )
    }
    let mut addresses = describe_addresses
        .send()
        .await?
        .addresses
        .ok_or(Error::MissingAddresses)?;

    let mut legacy_addresses = eip::describe_addresses_with_tag_value(
        ec2_client,
        eip::LEGACY_CLUSTER_NAME_TAG,
        cluster_name.to_owned(),
    )
    .await?
    .addresses
    .ok_or(Error::MissingAddresses)?;

    addresses.append(&mut legacy_addresses);

    let eip_uids: HashSet<String> = eip_api
        .list(&ListParams::default())
        .await?
        .into_iter()
        .filter_map(|eip| eip.metadata.uid)
        .collect();

    for address in addresses {
        let eip_uid = eip::get_tag_from_address(&address, eip::EIP_UID_TAG);
        if eip_uid.is_none() || !eip_uids.contains(eip_uid.unwrap()) {
            event!(Level::WARN,
                allocation_id = %address.allocation_id.as_deref().unwrap_or("None"),
                eip_uid = %eip_uid.unwrap_or("None"),
                "Cleaning up orphaned EIP",
            );
            eip::disassociate_and_release_address(ec2_client, &address).await?;
        }
    }

    // Manually remove the old finalizer, since we just removed the EIPs.
    // https://docs.rs/kube-runtime/0.65.0/src/kube_runtime/finalizer.rs.html#133
    let legacy_pods = pod_api
        .list(&ListParams::default().labels(LEGACY_MANAGE_EIP_LABEL))
        .await?;
    for pod in legacy_pods {
        if let Some(position) = pod
            .finalizers()
            .iter()
            .position(|s| s == LEGACY_POD_FINALIZER_NAME)
        {
            let pod_name = pod.meta().name.as_ref().ok_or(Error::MissingPodName)?;
            let finalizer_path = format!("/metadata/finalizers/{}", position);
            pod_api
                .patch::<Pod>(
                    pod_name,
                    &PatchParams::default(),
                    &Patch::Json(json_patch::Patch(vec![
                        PatchOperation::Test(TestOperation {
                            path: finalizer_path.clone(),
                            value: LEGACY_POD_FINALIZER_NAME.into(),
                        }),
                        PatchOperation::Remove(RemoveOperation {
                            path: finalizer_path,
                        }),
                    ])),
                )
                .await?;
        }
    }
    Ok(())
}

/// Takes actions to create/associate an EIP with the pod or clean up if the pod is being deleted.
/// Wraps these operations with a finalizer to ensure the pod is not deleted without cleaning up
/// the Elastic IP associated with it.
#[instrument(skip(pod, context), err)]
async fn reconcile_pod(
    pod: Arc<Pod>,
    context: Arc<ContextData>,
) -> Result<Action, kube_runtime::finalizer::Error<Error>> {
    let namespace = pod.namespace().unwrap();
    let k8s_client = context.k8s_client.clone();
    let pod_api: Api<Pod> = Api::namespaced(k8s_client.clone(), &namespace);
    let eip_api = Api::<Eip>::namespaced(k8s_client.clone(), &namespace);
    let node_api: Api<Node> = Api::all(k8s_client.clone());
    let ec2_client = context.ec2_client.clone();
    finalizer(&pod_api, POD_FINALIZER_NAME, pod, |event| async {
        match event {
            Event::Apply(pod) => apply_pod(&ec2_client, &node_api, &eip_api, &pod_api, pod).await,
            Event::Cleanup(pod) => cleanup_pod(&ec2_client, &eip_api, pod).await,
        }
    })
    .await
}

#[instrument(skip(node, context), err)]
async fn reconcile_node(
    node: Arc<Node>,
    context: Arc<ContextData>,
) -> Result<Action, kube_runtime::finalizer::Error<Error>> {
    let k8s_client = context.k8s_client.clone();
    let eip_api = Api::<Eip>::namespaced(
        k8s_client.clone(),
        context.namespace.as_deref().unwrap_or("default"),
    );
    let node_api: Api<Node> = Api::all(k8s_client.clone());
    let ec2_client = context.ec2_client.clone();
    finalizer(&node_api, NODE_FINALIZER_NAME, node, |event| async {
        match event {
            Event::Apply(node) => apply_node(&ec2_client, &eip_api, node).await,
            Event::Cleanup(node) => cleanup_node(&ec2_client, &eip_api, node).await,
        }
    })
    .await
}

/// Takes actions to create an EIP or clean up if the Eip K8S resource is being deleted.
/// Wraps these operations with a finalizer to ensure the K8S Eip is not deleted without
/// cleaning up the Elastic IP associated with it.
#[instrument(skip(eip, context), err)]
async fn reconcile_eip(
    eip: Arc<Eip>,
    context: Arc<ContextData>,
) -> Result<Action, kube_runtime::finalizer::Error<Error>> {
    let namespace = eip.namespace().unwrap();
    let cluster_name = &context.cluster_name;
    let default_tags = &context.default_tags;
    let k8s_client = context.k8s_client.clone();
    let eip_api = Api::<Eip>::namespaced(k8s_client.clone(), &namespace);
    let ec2_client = context.ec2_client.clone();
    finalizer(&eip_api, EIP_FINALIZER_NAME, eip, |event| async {
        match event {
            Event::Apply(eip) => {
                apply_eip(
                    &ec2_client,
                    &eip_api,
                    eip,
                    cluster_name,
                    &namespace,
                    default_tags,
                )
                .await
            }
            Event::Cleanup(eip) => cleanup_eip(&ec2_client, eip).await,
        }
    })
    .await
}

/// Requeues the operation if there is an error.
fn on_error<T>(
    _obj: Arc<T>,
    _error: &kube_runtime::finalizer::Error<Error>,
    _context: Arc<ContextData>,
) -> Action {
    Action::requeue(Duration::from_millis(thread_rng().gen_range(4000..8000)))
}

fn main() -> Result<(), Error> {
    let runtime = tokio::runtime::Builder::new_multi_thread()
        .enable_all()
        .build()?;
    runtime.block_on(run_with_tracing("eip_operator", run))?;
    Ok(())
}

#[instrument(skip(ec2_client, quota_client), err)]
async fn report_eip_quota_status(
    ec2_client: &Ec2Client,
    quota_client: &ServiceQuotaClient,
) -> Result<(), Error> {
    let addresses_result = ec2_client.describe_addresses().send().await?;
    let allocated = addresses_result.addresses().unwrap_or_default().len();
    let quota_result = quota_client
        .get_service_quota()
        .service_code("ec2")
        .quota_code(EIP_QUOTA_CODE)
        .send()
        .await?;
    let quota = quota_result
        .quota()
        .and_then(|q: &ServiceQuota| q.value)
        .unwrap_or(0f64);
    event!(Level::INFO, eips_allocated = %allocated, eip_quota = %quota, "eip_quota_checked");
    Ok(())
}

async fn run() -> Result<(), Error> {
    debug!("Getting k8s_client...");
    let k8s_client = Client::try_default().await?;

    debug!("Getting ec2_client...");
    let mut config_loader = aws_config::from_env();
    if let Ok(endpoint) = std::env::var("AWS_ENDPOINT_URL") {
        config_loader = config_loader.endpoint_resolver(AWSEndpoint::immutable(
            endpoint.parse().expect("{endpoint} not valid URI"),
        ))
    }
    let aws_config = config_loader.load().await;
    let ec2_client = Ec2Client::new(&aws_config);

    debug!("Getting quota_client...");
    let quota_client = ServiceQuotaClient::new(&aws_config);

    debug!("Getting namespace from env...");
    let namespace = std::env::var("NAMESPACE").ok();

    debug!("Getting cluster name from env...");
    let cluster_name =
        std::env::var("CLUSTER_NAME").expect("Environment variable CLUSTER_NAME is required.");

    debug!("Getting default tags from env...");
    let default_tags: HashMap<String, String> =
        serde_json::from_str(&std::env::var("DEFAULT_TAGS").unwrap_or_else(|_| "{}".to_owned()))?;

    register_eip_custom_resource(k8s_client.clone(), namespace.as_deref()).await?;

    debug!("Getting pod api");
    let pod_api = match namespace {
        Some(ref namespace) => Api::<Pod>::namespaced(k8s_client.clone(), namespace),
        None => Api::<Pod>::all(k8s_client.clone()),
    };

    debug!("Getting node api");
    let node_api = Api::<Node>::all(k8s_client.clone());

    debug!("Getting eip api");
    let eip_api = match namespace {
        Some(ref namespace) => Api::<Eip>::namespaced(k8s_client.clone(), namespace),
        None => Api::<Eip>::all(k8s_client.clone()),
    };

    debug!("Cleaning up any orphaned EIPs");
    cleanup_orphan_eips(
        &ec2_client,
        &eip_api,
        &pod_api,
        &cluster_name,
        namespace.as_deref(),
    )
    .await?;

    info!("Starting quota watcher");
    // Note: cloning EC2 client so it can be moved into the interval task
    let ec3_client = ec2_client.clone();
    // Intentionally capturing this, for ease of debugging, but not doing anything with
    // it; Tokio should drop the task when it goes out of scope
    let _quota_watcher = task::spawn(async move {
        let mut interval = tokio::time::interval(EIP_QUOTA_INTERVAL);
        // It's better to miss the occasional measurement than to hammer the endpoint
        interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);

        loop {
            interval.tick().await;
            // Note: the Err that might occur here will be handled by tracing
            // instrumentation, rather than directly here.
            if let Err(err) = report_eip_quota_status(&ec3_client, &quota_client).await {
                event!(Level::ERROR, err = %err, "Quota reporting error");
            }
        }
    });

    info!("Watching for events...");
    let context = Arc::new(ContextData::new(
        cluster_name,
        namespace,
        default_tags,
        k8s_client.clone(),
        ec2_client,
    ));
    let pod_controller = Controller::new(pod_api, ListParams::default().labels(MANAGE_EIP_LABEL))
        .run(reconcile_pod, on_error, context.clone())
        .for_each(|reconciliation_result| async move {
            match reconciliation_result {
                Ok(resource) => {
                    event!(Level::INFO, pod_name = %resource.0.name, "Pod reconciliation successful.");
                }
                Err(err) => event!(Level::ERROR, err = %err, "Pod reconciliation error."),
            }
        });

    let node_controller = Controller::new(node_api, ListParams::default().labels(MANAGE_EIP_LABEL))
        .run(reconcile_node, on_error, context.clone())
        .for_each(|reconciliation_result| async move {
            match reconciliation_result {
                Ok(resource) => {
                    event!(Level::INFO, node_name = %resource.0.name, "Node reconciliation successful.");
                }
                Err(err) => event!(Level::ERROR, err = %err, "Node reconciliation error."),
            }
        });

    let eip_controller = Controller::new(eip_api, ListParams::default())
        .run(reconcile_eip, on_error, context)
        .for_each(|reconciliation_result| async move {
            match reconciliation_result {
                Ok(resource) => {
                    event!(Level::INFO, eip_name = %resource.0.name, "EIP reconciliation successful.");
                }
                Err(err) => event!(Level::ERROR, err = %err, "EIP reconciliation error."),
            }
        });
    join!(pod_controller, node_controller, eip_controller);
    debug!("exiting");
    Ok(())
}
